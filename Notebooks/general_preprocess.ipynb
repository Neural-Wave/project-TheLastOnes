{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    'B-ACCOUNTNUM',\n",
    "    'B-BUILDINGNUM',\n",
    "    'B-CITY',\n",
    "    'B-CREDITCARDNUMBER',\n",
    "    'B-DATEOFBIRTH',\n",
    "    'B-DRIVERLICENSENUM',\n",
    "    'B-EMAIL',\n",
    "    'B-GIVENNAME',\n",
    "    'B-IDCARDNUM',\n",
    "    'B-PASSWORD',\n",
    "    'B-SOCIALNUM',\n",
    "    'B-STREET',\n",
    "    'B-SURNAME',\n",
    "    'B-TAXNUM',\n",
    "    'B-TELEPHONENUM',\n",
    "    'B-USERNAME',\n",
    "    'B-ZIPCODE',\n",
    "    'I-ACCOUNTNUM',\n",
    "    'I-BUILDINGNUM',\n",
    "    'I-CITY',\n",
    "    'I-CREDITCARDNUMBER',\n",
    "    'I-DATEOFBIRTH',\n",
    "    'I-DRIVERLICENSENUM',\n",
    "    'I-EMAIL',\n",
    "    'I-GIVENNAME',\n",
    "    'I-IDCARDNUM',\n",
    "    'I-PASSWORD',\n",
    "    'I-SOCIALNUM',\n",
    "    'I-STREET',\n",
    "    'I-SURNAME',\n",
    "    'I-TAXNUM',\n",
    "    'I-TELEPHONENUM',\n",
    "    'I-USERNAME',\n",
    "    'I-ZIPCODE',\n",
    "    'O',\n",
    "]\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "label_set = set(l[2:] for l in label_list[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_labels(text, privacy_mask):\n",
    "    # sort privacy mask by start position\n",
    "    privacy_mask = sorted(privacy_mask, key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    # replace sensitive pieces of text with labels\n",
    "    for item in privacy_mask:\n",
    "        label = item['label']\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        value = item['value']\n",
    "        # count the number of words in the value\n",
    "        word_count = len(value.split())\n",
    "        \n",
    "        # replace the sensitive information with the appropriate number of [label] placeholders\n",
    "        replacement = \" \".join([f\"{label}\" for _ in range(word_count)])\n",
    "        text = text[:start] + replacement + text[end:]\n",
    "        \n",
    "    words = text.split()\n",
    "    # assign labels to each word\n",
    "    labels = []\n",
    "    for word in words:\n",
    "        match = re.search(r\"(\\w+)\", word)  # match any word character\n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "            if label in label_set:\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                # any other word is labeled as \"O\"\n",
    "                labels.append(\"O\")\n",
    "        else:\n",
    "            labels.append(\"O\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "def tokenize_and_align_labels(examples):\n",
    "    words = [t.split() for t in examples[\"source_text\"]]\n",
    "    tokenized_inputs = tokenizer(words, truncation=True, is_split_into_words=True, max_length=512)\n",
    "    source_labels = [\n",
    "        generate_sequence_labels(text, mask)\n",
    "        for text, mask in zip(examples[\"source_text\"], examples[\"privacy_mask\"])\n",
    "    ]\n",
    "\n",
    "    labels = []\n",
    "    valid_idx = []\n",
    "    for i, label in enumerate(source_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # map tokens to their respective word.\n",
    "        previous_label = None\n",
    "        label_ids = [-100]\n",
    "        try:\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    continue\n",
    "                elif label[word_idx] == \"O\":\n",
    "                    label_ids.append(label2id[\"O\"])\n",
    "                    continue\n",
    "                elif previous_label == label[word_idx]:\n",
    "                    label_ids.append(label2id[f\"I-{label[word_idx]}\"])\n",
    "                else:\n",
    "                    label_ids.append(label2id[f\"B-{label[word_idx]}\"])\n",
    "                previous_label = label[word_idx]\n",
    "            label_ids = label_ids[:511] + [-100]\n",
    "            labels.append(label_ids)\n",
    "            # print(word_ids)\n",
    "            # print(label_ids)\n",
    "        except:\n",
    "            global k\n",
    "            k += 1\n",
    "            # print(f\"{word_idx = }\")\n",
    "            # print(f\"{len(label) = }\")\n",
    "            labels.append([-100] * len(tokenized_inputs[\"input_ids\"][i]))\n",
    "        \"\"\"\n",
    "        except:\n",
    "            print(f\"{word_ids[-2] = }\")\n",
    "            print(f\"{len(label) = }\")\n",
    "            print(\"Unvalid data detected\")\n",
    "            labels.append([-100] * len(word_ids))\n",
    "        \"\"\"\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of invalid data\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\", truncation=True, max_length=512)\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-400k\")\n",
    "train = dataset[\"train\"]\n",
    "valid = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b554a363d441a08e7494d20e1cfa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/325517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fd9a7a54654fab99594cd367db5928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_train = train.map(tokenize_and_align_labels, batched=True)\n",
    "token_valid = valid.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042e015884844cc08b1f28e20d359151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/325517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52d163cc0f64a2fa72288176d1b569b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/81379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": token_train,\n",
    "    \"validation\": token_valid\n",
    "})\n",
    "\n",
    "tokenized_datasets.save_to_disk(\"./tokenized_dataset/gen_tokenized_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\", truncation=True, max_length=512)\n",
    "train = load_from_disk(\"./tokenized_dataset/gen_tokenized_data\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenizer.convert_ids_to_tokens(train[0][\"source_text\"])\n",
    "tokens = tokenizer(train[0])\n",
    "labels = train[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels) == len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: '▁<';\tlabel: 'O'\n",
      "token: 'p';\tlabel: 'O'\n",
      "token: '>';\tlabel: 'O'\n",
      "token: 'My';\tlabel: 'O'\n",
      "token: '▁child';\tlabel: 'O'\n",
      "token: '▁fao';\tlabel: 'B-USERNAME'\n",
      "token: 'zz';\tlabel: 'I-USERNAME'\n",
      "token: 's';\tlabel: 'I-USERNAME'\n",
      "token: 'd';\tlabel: 'I-USERNAME'\n",
      "token: '379';\tlabel: 'I-USERNAME'\n",
      "token: '223';\tlabel: 'I-USERNAME'\n",
      "token: '▁(';\tlabel: 'O'\n",
      "token: 'DOB';\tlabel: 'O'\n",
      "token: ':';\tlabel: 'O'\n",
      "token: '▁May';\tlabel: 'B-DATEOFBIRTH'\n",
      "token: '/';\tlabel: 'I-DATEOFBIRTH'\n",
      "token: '58)';\tlabel: 'I-DATEOFBIRTH'\n",
      "token: '▁will';\tlabel: 'O'\n",
      "token: '▁under';\tlabel: 'O'\n",
      "token: 'go';\tlabel: 'O'\n",
      "token: '▁treatment';\tlabel: 'O'\n",
      "token: '▁with';\tlabel: 'O'\n",
      "token: '▁Dr';\tlabel: 'O'\n",
      "token: '.';\tlabel: 'O'\n",
      "token: '▁fao';\tlabel: 'B-USERNAME'\n",
      "token: 'zz';\tlabel: 'I-USERNAME'\n",
      "token: 's';\tlabel: 'I-USERNAME'\n",
      "token: 'd';\tlabel: 'I-USERNAME'\n",
      "token: '379';\tlabel: 'I-USERNAME'\n",
      "token: '223';\tlabel: 'I-USERNAME'\n",
      "token: ',';\tlabel: 'I-USERNAME'\n",
      "token: '▁office';\tlabel: 'O'\n",
      "token: '▁at';\tlabel: 'O'\n",
      "token: '▁Hill';\tlabel: 'B-STREET'\n",
      "token: '▁Road';\tlabel: 'I-STREET'\n",
      "token: '.';\tlabel: 'I-STREET'\n",
      "token: '▁Our';\tlabel: 'O'\n",
      "token: '▁';\tlabel: 'O'\n",
      "token: 'ZIP';\tlabel: 'O'\n",
      "token: '▁code';\tlabel: 'O'\n",
      "token: '▁is';\tlabel: 'O'\n",
      "token: '▁';\tlabel: 'B-ZIPCODE'\n",
      "token: '2817';\tlabel: 'I-ZIPCODE'\n",
      "token: '0-6';\tlabel: 'I-ZIPCODE'\n",
      "token: '392';\tlabel: 'I-ZIPCODE'\n",
      "token: '.';\tlabel: 'I-ZIPCODE'\n",
      "token: '▁Consult';\tlabel: 'O'\n",
      "token: '▁policy';\tlabel: 'O'\n",
      "token: '▁M';\tlabel: 'O'\n",
      "token: '.';\tlabel: 'O'\n",
      "token: 'UE';\tlabel: 'O'\n",
      "token: '.';\tlabel: 'O'\n",
      "token: '227';\tlabel: 'O'\n",
      "token: '995';\tlabel: 'O'\n",
      "token: '.';\tlabel: 'O'\n",
      "token: '▁Contact';\tlabel: 'O'\n",
      "token: '▁number';\tlabel: 'O'\n",
      "token: ':';\tlabel: 'O'\n",
      "token: '▁007';\tlabel: 'B-TELEPHONENUM'\n",
      "token: '0.60';\tlabel: 'I-TELEPHONENUM'\n",
      "token: '6.3';\tlabel: 'I-TELEPHONENUM'\n",
      "token: '22.';\tlabel: 'I-TELEPHONENUM'\n",
      "token: '6244';\tlabel: 'I-TELEPHONENUM'\n",
      "token: '.';\tlabel: 'I-TELEPHONENUM'\n",
      "token: '▁Handle';\tlabel: 'O'\n",
      "token: '▁';\tlabel: 'O'\n",
      "token: 'transactions';\tlabel: 'O'\n",
      "token: '▁with';\tlabel: 'O'\n",
      "token: '▁6';\tlabel: 'B-CREDITCARDNUMBER'\n",
      "token: '2254';\tlabel: 'I-CREDITCARDNUMBER'\n",
      "token: '2722';\tlabel: 'I-CREDITCARDNUMBER'\n",
      "token: '0412';\tlabel: 'I-CREDITCARDNUMBER'\n",
      "token: '963.';\tlabel: 'I-CREDITCARDNUMBER'\n",
      "token: '▁Que';\tlabel: 'O'\n",
      "token: 'ries';\tlabel: 'O'\n",
      "token: '?';\tlabel: 'O'\n",
      "token: '▁Email';\tlabel: 'O'\n",
      "token: ':';\tlabel: 'O'\n",
      "token: '▁fao';\tlabel: 'B-EMAIL'\n",
      "token: 'zz';\tlabel: 'I-EMAIL'\n",
      "token: 's';\tlabel: 'I-EMAIL'\n",
      "token: 'd';\tlabel: 'I-EMAIL'\n",
      "token: '379';\tlabel: 'I-EMAIL'\n",
      "token: '223';\tlabel: 'I-EMAIL'\n",
      "token: '@';\tlabel: 'I-EMAIL'\n",
      "token: 'outlook';\tlabel: 'I-EMAIL'\n",
      "token: '.';\tlabel: 'I-EMAIL'\n",
      "token: 'com';\tlabel: 'I-EMAIL'\n",
      "token: '.</';\tlabel: 'I-EMAIL'\n",
      "token: 'p';\tlabel: 'I-EMAIL'\n",
      "token: '>';\tlabel: 'I-EMAIL'\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(train[0][\"labels\"])-1): \n",
    "    print(f\"token: '{tokens[i]}';\\tlabel: '{id2label[labels[i]]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
