{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of entity labels and create mappings between labels and IDs\n",
    "label_list = [\n",
    "    'B-ACCOUNTNUM',\n",
    "    'B-BUILDINGNUM',\n",
    "    'B-CITY',\n",
    "    'B-CREDITCARDNUMBER',\n",
    "    'B-DATEOFBIRTH',\n",
    "    'B-DRIVERLICENSENUM',\n",
    "    'B-EMAIL',\n",
    "    'B-GIVENNAME',\n",
    "    'B-IDCARDNUM',\n",
    "    'B-PASSWORD',\n",
    "    'B-SOCIALNUM',\n",
    "    'B-STREET',\n",
    "    'B-SURNAME',\n",
    "    'B-TAXNUM',\n",
    "    'B-TELEPHONENUM',\n",
    "    'B-USERNAME',\n",
    "    'B-ZIPCODE',\n",
    "    'I-ACCOUNTNUM',\n",
    "    'I-BUILDINGNUM',\n",
    "    'I-CITY',\n",
    "    'I-CREDITCARDNUMBER',\n",
    "    'I-DATEOFBIRTH',\n",
    "    'I-DRIVERLICENSENUM',\n",
    "    'I-EMAIL',\n",
    "    'I-GIVENNAME',\n",
    "    'I-IDCARDNUM',\n",
    "    'I-PASSWORD',\n",
    "    'I-SOCIALNUM',\n",
    "    'I-STREET',\n",
    "    'I-SURNAME',\n",
    "    'I-TAXNUM',\n",
    "    'I-TELEPHONENUM',\n",
    "    'I-USERNAME',\n",
    "    'I-ZIPCODE',\n",
    "    'O',\n",
    "]\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the privacy dataset\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-400k\")\n",
    "training_set = dataset[\"train\"]\n",
    "valid_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': Value(dtype='string', id=None),\n",
       " 'locale': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'privacy_mask': [{'label': Value(dtype='string', id=None),\n",
       "   'start': Value(dtype='int64', id=None),\n",
       "   'end': Value(dtype='int64', id=None),\n",
       "   'value': Value(dtype='string', id=None),\n",
       "   'label_index': Value(dtype='int64', id=None)}],\n",
       " 'uid': Value(dtype='int64', id=None),\n",
       " 'masked_text': Value(dtype='string', id=None),\n",
       " 'mbert_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'mbert_token_classes': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " 'p',\n",
       " '>',\n",
       " 'My',\n",
       " 'child',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " '(',\n",
       " 'DO',\n",
       " '##B',\n",
       " ':',\n",
       " 'May',\n",
       " '/',\n",
       " '58',\n",
       " ')',\n",
       " 'will',\n",
       " 'under',\n",
       " '##go',\n",
       " 'treatment',\n",
       " 'with',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " ',',\n",
       " 'office',\n",
       " 'at',\n",
       " 'Hill',\n",
       " 'Road',\n",
       " '.',\n",
       " 'Our',\n",
       " 'Z',\n",
       " '##IP',\n",
       " 'code',\n",
       " 'is',\n",
       " '281',\n",
       " '##70',\n",
       " '-',\n",
       " '639',\n",
       " '##2',\n",
       " '.',\n",
       " 'Con',\n",
       " '##sul',\n",
       " '##t',\n",
       " 'policy',\n",
       " 'M',\n",
       " '.',\n",
       " 'UE',\n",
       " '.',\n",
       " '227',\n",
       " '##99',\n",
       " '##5',\n",
       " '.',\n",
       " 'Contact',\n",
       " 'number',\n",
       " ':',\n",
       " '007',\n",
       " '##0',\n",
       " '.',\n",
       " '606',\n",
       " '.',\n",
       " '322',\n",
       " '.',\n",
       " '624',\n",
       " '##4',\n",
       " '.',\n",
       " 'Hand',\n",
       " '##le',\n",
       " 'transaction',\n",
       " '##s',\n",
       " 'with',\n",
       " '622',\n",
       " '##5',\n",
       " '##42',\n",
       " '##7',\n",
       " '##22',\n",
       " '##04',\n",
       " '##12',\n",
       " '##9',\n",
       " '##6',\n",
       " '##3',\n",
       " '.',\n",
       " 'Que',\n",
       " '##ries',\n",
       " '?',\n",
       " 'Em',\n",
       " '##ail',\n",
       " ':',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " '@',\n",
       " 'out',\n",
       " '##lo',\n",
       " '##ok',\n",
       " '.',\n",
       " 'com',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = training_set[0]['source_text']\n",
    "training_set[0][\"mbert_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'My',\n",
       " 'child',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " '(',\n",
       " 'DO',\n",
       " '##B',\n",
       " ':',\n",
       " 'May',\n",
       " '/',\n",
       " '58',\n",
       " ')',\n",
       " 'will',\n",
       " 'under',\n",
       " '##go',\n",
       " 'treatment',\n",
       " 'with',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " ',',\n",
       " 'office',\n",
       " 'at',\n",
       " 'Hill',\n",
       " 'Road',\n",
       " '.',\n",
       " 'Our',\n",
       " 'Z',\n",
       " '##IP',\n",
       " 'code',\n",
       " 'is',\n",
       " '281',\n",
       " '##70',\n",
       " '-',\n",
       " '639',\n",
       " '##2',\n",
       " '.',\n",
       " 'Con',\n",
       " '##sul',\n",
       " '##t',\n",
       " 'policy',\n",
       " 'M',\n",
       " '.',\n",
       " 'UE',\n",
       " '.',\n",
       " '227',\n",
       " '##99',\n",
       " '##5',\n",
       " '.',\n",
       " 'Contact',\n",
       " 'number',\n",
       " ':',\n",
       " '007',\n",
       " '##0',\n",
       " '.',\n",
       " '606',\n",
       " '.',\n",
       " '322',\n",
       " '.',\n",
       " '624',\n",
       " '##4',\n",
       " '.',\n",
       " 'Hand',\n",
       " '##le',\n",
       " 'transaction',\n",
       " '##s',\n",
       " 'with',\n",
       " '622',\n",
       " '##5',\n",
       " '##42',\n",
       " '##7',\n",
       " '##22',\n",
       " '##04',\n",
       " '##12',\n",
       " '##9',\n",
       " '##6',\n",
       " '##3',\n",
       " '.',\n",
       " 'Que',\n",
       " '##ries',\n",
       " '?',\n",
       " 'Em',\n",
       " '##ail',\n",
       " ':',\n",
       " 'fa',\n",
       " '##oz',\n",
       " '##zs',\n",
       " '##d',\n",
       " '##3',\n",
       " '##7',\n",
       " '##9',\n",
       " '##22',\n",
       " '##3',\n",
       " '@',\n",
       " 'out',\n",
       " '##lo',\n",
       " '##ok',\n",
       " '.',\n",
       " 'com',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(example, truncation=True)\n",
    "tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "# tokens are identical except the first and the last special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align label and tokens\n",
    "def align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"source_text\"], truncation=True)\n",
    "    label_ids = [-100]\n",
    "\n",
    "    for privacy_class in examples[\"mbert_token_classes\"]:\n",
    "        label_ids.append(label2id[privacy_class])\n",
    "    label_ids = label_ids[:511]  # truncate too long labels\n",
    "    label_ids.append(-100)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_training_set = training_set.map(align_labels, batched=False)\n",
    "tokenized_valid_set = valid_set.map(align_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns = [\n",
    "    'locale',\n",
    "    'split',\n",
    "    'privacy_mask',\n",
    "    'uid',\n",
    "    'mbert_tokens',\n",
    "    'mbert_token_classes',\n",
    "]\n",
    "\n",
    "tokenized_training_set = tokenized_training_set.remove_columns(removed_columns)\n",
    "tokenized_valid_set = tokenized_valid_set.remove_columns(removed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ec9ab88ab445fbde03962ab2b02c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/325517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9883a69fd764cc984b9fa2424f675e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/81379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_training_set,\n",
    "    \"validation\": tokenized_valid_set\n",
    "})\n",
    "\n",
    "tokenized_datasets.save_to_disk(\"./tokenized_dataset/tokenized_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
